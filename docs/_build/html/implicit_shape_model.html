

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Implicit Shape Model &mdash; PCL DOCUMENTATION 0.0.1 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: dark blue" >
          

          
            <a href="index.html" class="icon icon-home"> PCL DOCUMENTATION
          

          
          </a>

          
            
            
              <div class="version">
                0.0.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Basic Usage</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="walkthrough.html">1. PCL Walkthrough</a></li>
<li class="toctree-l1"><a class="reference internal" href="basic_structures.html">2. Getting Started / Basic Structures</a></li>
<li class="toctree-l1"><a class="reference internal" href="using_pcl_pcl_config.html">3. Using PCL in your own project</a></li>
<li class="toctree-l1"><a class="reference internal" href="compiling_pcl_posix.html">4. Compiling PCL from source on POSIX compliant systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="building_pcl.html">5. Customizing the PCL build process</a></li>
<li class="toctree-l1"><a class="reference internal" href="compiling_pcl_dependencies_windows.html">6. Building PCL’s dependencies from source on Windows</a></li>
<li class="toctree-l1"><a class="reference internal" href="compiling_pcl_windows.html">7. Compiling PCL from source on Windows</a></li>
<li class="toctree-l1"><a class="reference internal" href="compiling_pcl_macosx.html">8. Compiling PCL and its dependencies from MacPorts and source on Mac OS X</a></li>
<li class="toctree-l1"><a class="reference internal" href="installing_homebrew.html">9. Installing on Mac OS X using Homebrew</a></li>
<li class="toctree-l1"><a class="reference internal" href="using_pcl_with_eclipse.html">10. Using PCL with Eclipse</a></li>
<li class="toctree-l1"><a class="reference internal" href="generate_local_doc.html">11. Generate a local documentation for PCL</a></li>
<li class="toctree-l1"><a class="reference internal" href="matrix_transform.html">12. Using a matrix to transform a point cloud</a></li>
</ul>
<p class="caption"><span class="caption-text">Advanced Usage</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="adding_custom_ptype.html">1. Adding your own custom <cite>PointT</cite> type</a></li>
<li class="toctree-l1"><a class="reference internal" href="writing_new_classes.html">2. Writing a new PCL class</a></li>
</ul>
<p class="caption"><span class="caption-text">Features</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="how_features_work.html">1. How 3D Features work in PCL</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">PCL DOCUMENTATION</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Implicit Shape Model</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/implicit_shape_model.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="implicit-shape-model">
<span id="id1"></span><h1>Implicit Shape Model<a class="headerlink" href="#implicit-shape-model" title="Permalink to this headline">¶</a></h1>
<p>In this tutorial we will learn how to use the implicit shape model algorithm implemented in the <code class="docutils literal notranslate"><span class="pre">pcl::ism::ImplicitShapeModel</span></code> class.
This algorithm was described in the article <a class="reference external" href="http://homes.esat.kuleuven.be/~jknopp/papers/2010eccv_3d_paper.pdf">“Hough Transforms and 3D SURF for robust three dimensional classification”</a> by Jan Knopp, Mukta Prasad, Geert Willems, Radu Timofte, and Luc Van Gool.
This algorithm is a combination of generalized Hough transform and the Bag of Features approach and its purpose is as follows. Having some training set - point clouds of different objects of the known class - the algorithm computes a certain model which will be later used to predict an object center in the given cloud that wasn’t a part of the training set.</p>
</div>
<div class="section" id="theoretical-primer">
<h1>Theoretical Primer<a class="headerlink" href="#theoretical-primer" title="Permalink to this headline">¶</a></h1>
<dl>
<dt>The algorithm consists of two steps, the first one is training, and the second is recognition of the objects in the clouds that weren’t in the training set. Let’s take a look at how the training is done. It consists of six steps:</dt><dd><ol class="arabic">
<li><p>First of all the keypoint detection is made. In the given implementation it’s just a simplification of the training clouds. At this step all the point clouds are simplified by the means of the voxel grid approach; remaining points are declared as keypoints.</p></li>
<li><p>For every keypoint features are estimated. In the example below the FPFH estimation is used.</p></li>
<li><p>All features are clustered with the help of k-means algorithm to construct a dictionary of visual (or geometric) words. Obtained clusters represent visual words. Every feature in the cluster is the instance of this visual word.</p></li>
<li><p>For every single instance the direction to center is computed - a direction from the keypoint (from which the feature was obtained) to the center of mass of the given cloud.</p></li>
<li><p>For each visual word the statistical weight is calculated by the formula:</p>
<p class="centered">
<strong><span class="math notranslate nohighlight">\(W_{st}(c_i,v_j)=\frac{1}{n_{vw}(c_i)} \frac{1}{n_{vot}(v_j)} \frac{\frac{n_{vot}(c_i,v_j)}{n_{ftr}(c_i)}}{\sum_{c_k\in C}\frac{n_{vot}(c_k,v_j)}{n_{ftr}(c_k)}}\)</span></strong></p><p>The statistical weight <span class="math notranslate nohighlight">\(W_{st}(c_i,v_j)\)</span> weights all the votes cast by visual word <span class="math notranslate nohighlight">\(v_j\)</span> for class <span class="math notranslate nohighlight">\(c_i\)</span>. Here <span class="math notranslate nohighlight">\(n_{vot}(v_j)\)</span> is the total number of votes from visual word <span class="math notranslate nohighlight">\(v_j\)</span>, <span class="math notranslate nohighlight">\(n_{vot}(c_i,v_j)\)</span> is the number of votes for class <span class="math notranslate nohighlight">\(c_i\)</span> from <span class="math notranslate nohighlight">\(v_j\)</span>, <span class="math notranslate nohighlight">\(n_{vw}(c_i)\)</span> is the number of visual words that vote for class <span class="math notranslate nohighlight">\(c_i\)</span>, <span class="math notranslate nohighlight">\(n_{ftr}(c_i)\)</span> is the number of features from which <span class="math notranslate nohighlight">\(c_i\)</span> was learned. <span class="math notranslate nohighlight">\(C\)</span> is the set of all classes.</p>
</li>
<li><p>For every keypoint (point for which feature was estimated) the learned weight is calculated by the formula:</p>
<p class="centered">
<strong><span class="math notranslate nohighlight">\(W_{lrn}(\lambda_{ij})=f(\{e^{-\frac{{d_a(\lambda_{ij})}^2}{\sigma^2}} \mid a \in A\})\)</span></strong></p><p>Authors of the article define <span class="math notranslate nohighlight">\(\lambda_{ij}\)</span> as the vote cast by a particular instance of visual word <span class="math notranslate nohighlight">\(v_j\)</span> on a particular training shape of class <span class="math notranslate nohighlight">\(c_i\)</span>; that is, <span class="math notranslate nohighlight">\(\lambda_{ij}\)</span> records the distance of the particular instance of visual word <span class="math notranslate nohighlight">\(v_j\)</span> to the center of the training shape on which it was found. Here <span class="math notranslate nohighlight">\(A\)</span> is the set of all features associated with word <span class="math notranslate nohighlight">\(v_j\)</span> on a shape of class <span class="math notranslate nohighlight">\(c_i\)</span>. The recommend value for <span class="math notranslate nohighlight">\(\sigma\)</span> is 10% of the shape size. Function <span class="math notranslate nohighlight">\(f\)</span> is simply a median. <span class="math notranslate nohighlight">\(d_a(\lambda_{ij})\)</span> is the Euclidean distance between voted and actual center.</p>
</li>
</ol>
</dd>
<dt>After the training process is done and the trained model (weights, directions etc.) is obtained, the process of object search (or recognition) takes place. It consists of next four steps:</dt><dd><ol class="arabic">
<li><p>Keypoint detection.</p></li>
<li><p>Feature estimation for every keypoint of the cloud.</p></li>
<li><p>For each feature the search for the nearest visual word (that is a cluster) in the dictionary is made.</p></li>
<li><p>For every feature</p>
<ul>
<li><p>For every instance(which casts a vote for the class of interest) of every visual word from the trained model</p>
<ul>
<li><p>Add vote with the corresponding direction and vote power computed by the formula</p>
<p class="centered">
<strong><span class="math notranslate nohighlight">\(W(\lambda_{ij})=W_{st}(v_j,c_i) * W_{lrn}(\lambda_{ij})\)</span></strong></p></li>
</ul>
</li>
</ul>
</li>
<li><p>Previous step gives us a set of directions to the expected center and the power for each vote. In order to get single point that corresponds to center these votes need to be analysed. For this purpose algorithm uses the non maxima suppression approach. User just needs to pass the radius of the object of interest and the rest will be done by the <code class="docutils literal notranslate"><span class="pre">ISMVoteList::findStrongestPeaks</span> <span class="pre">()</span></code> method.</p></li>
</ol>
</dd>
</dl>
<p>For more comprehensive information please refer to the article
<a class="reference external" href="http://homes.esat.kuleuven.be/~jknopp/papers/2010eccv_3d_paper.pdf">“Hough Transforms and 3D SURF for robust three dimensional classification”</a>.</p>
</div>
<div class="section" id="the-code">
<h1>The code<a class="headerlink" href="#the-code" title="Permalink to this headline">¶</a></h1>
<p>First of all you will need the set of point clouds for this tutorial - training set and set of clouds for recognition.
Below is the list of clouds that are well suited for this tutorial (they were borrowed from the Ohio dataset).</p>
<dl class="simple">
<dt>Clouds for training:</dt><dd><ul class="simple">
<li><p><a class="reference external" href="https://raw.github.com/PointCloudLibrary/data/master/tutorials/ism_train_cat.pcd">Cat (train)</a></p></li>
<li><p><a class="reference external" href="https://raw.github.com/PointCloudLibrary/data/master/tutorials/ism_train_horse.pcd">Horse (train)</a></p></li>
<li><p><a class="reference external" href="https://raw.github.com/PointCloudLibrary/data/master/tutorials/ism_train_lioness.pcd">Lioness (train)</a></p></li>
<li><p><a class="reference external" href="https://raw.github.com/PointCloudLibrary/data/master/tutorials/ism_train_michael.pcd">Michael (train)</a></p></li>
<li><p><a class="reference external" href="https://raw.github.com/PointCloudLibrary/data/master/tutorials/ism_train_wolf.pcd">Wolf (train)</a></p></li>
</ul>
</dd>
<dt>Clouds for testing:</dt><dd><ul class="simple">
<li><p><a class="reference external" href="https://raw.github.com/PointCloudLibrary/data/master/tutorials/ism_test_cat.pcd">Cat</a></p></li>
<li><p><a class="reference external" href="https://raw.github.com/PointCloudLibrary/data/master/tutorials/ism_test_horse.pcd">Horse</a></p></li>
<li><p><a class="reference external" href="https://raw.github.com/PointCloudLibrary/data/master/tutorials/ism_test_lioness.pcd">Lioness</a></p></li>
<li><p><a class="reference external" href="https://raw.github.com/PointCloudLibrary/data/master/tutorials/ism_test_michael.pcd">Michael</a></p></li>
<li><p><a class="reference external" href="https://raw.github.com/PointCloudLibrary/data/master/tutorials/ism_test_wolf.pcd">Wolf</a></p></li>
</ul>
</dd>
</dl>
<p>Next what you need to do is to create a file <code class="docutils literal notranslate"><span class="pre">implicit_shape_model.cpp</span></code> in any editor you prefer and copy the following code inside of it:</p>
</div>
<div class="section" id="the-explanation">
<h1>The explanation<a class="headerlink" href="#the-explanation" title="Permalink to this headline">¶</a></h1>
<p>Now let’s study out what is the purpose of this code. The first lines of interest are these:</p>
<p>These lines simply load the clouds that will be used for training. Algorithm requires normals so this is the place where they are computed.
After the loop is passed all clouds will be inserted  to the training_clouds vector. training_normals and training_classes will store normals and class index for the corresponding object.</p>
<p>Here the instance of feature estimator is created, in our case it is the FPFH. It must be fully set up before it will be passed to the ISM algorithm. So this is the place where we define all feature estimation settings.</p>
<p>This line simply creates an instance of the <code class="docutils literal notranslate"><span class="pre">pcl::ism::ImplicitShapeModelEstimation</span></code>. It is a template class that has three parameters:</p>
<blockquote>
<div><ul class="simple">
<li><p>FeatureSize - size of the features (histograms) to compute</p></li>
<li><p>PointT - type of points to work with</p></li>
<li><p>NormalT - type of normals to use</p></li>
</ul>
</div></blockquote>
<p>Here the instance is provided with the training data and feature estimator. The last line provides sampling size value used for cloud simplification as mentioned before.</p>
<p>These lines simply launch the training process.</p>
<p>Here the trained model that was obtained during the training process is saved to file for possible reuse.</p>
<p>The remaining part of the code may be moved with a few changes to another .cpp file and be presented as a separate program that is responsible for classification.</p>
<p>This line loads trained model from file. It is not necessary, because we already have the trained model. It is given to show how you can load the precomputed model.</p>
<p>The classification process needs the cloud and its normals as well as the training process. So these lines simply load the cloud and compute normals.</p>
<p>This line launches the classification process. It tells the algorithm to look for the objects of type <code class="docutils literal notranslate"><span class="pre">testing_class</span></code> in the given cloud <code class="docutils literal notranslate"><span class="pre">testing_cloud</span></code>. Notice that the algorithm will use any trained model that you will pass. After the classification is done, the list of votes for center will be returned. <code class="docutils literal notranslate"><span class="pre">pcl::ism::ISMVoteList</span></code> is the separate class, which purpose is to help you to analyze the votes.</p>
<p>These lines are responsible for finding strongest peaks among the votes. This search is based on the non-maximum suppression idea, that’s why the non-maximum radius is equal to the object radius that is taken from the trained model.</p>
<p>The rest of the code is simple enough. It is responsible for visualizing the cloud and computed strongest peaks which represent the estimated centers of the object of type <code class="docutils literal notranslate"><span class="pre">testing_class</span></code>.</p>
</div>
<div class="section" id="compiling-and-running-the-program">
<h1>Compiling and running the program<a class="headerlink" href="#compiling-and-running-the-program" title="Permalink to this headline">¶</a></h1>
<p>Add the following lines to your CMakeLists.txt file:</p>
<p>Note that here we tell the compiler that we want a release version of the binaries, because the process of training is too slow.
After you have made the executable, you can run it. Simply do:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ ./implicit_shape_model
      ism_train_cat.pcd      0
      ism_train_horse.pcd    1
      ism_train_lioness.pcd  2
      ism_train_michael.pcd  3
      ism_train_wolf.pcd     4
      ism_test_cat.pcd       0
</pre></div>
</div>
<p>Here you must pass the training clouds and the class of the object that it contains. The last two parameters are the cloud for testing and the class of interest that you are looking for in the testing cloud.</p>
<p>After the segmentation the cloud viewer window will be opened and you will see something similar to those images:</p>
<a class="reference internal image-reference" href="_images/ism_tutorial_1.png"><img alt="_images/ism_tutorial_1.png" src="_images/ism_tutorial_1.png" style="height: 180px;" /></a>
<a class="reference internal image-reference" href="_images/ism_tutorial_2.png"><img alt="_images/ism_tutorial_2.png" src="_images/ism_tutorial_2.png" style="height: 360px;" /></a>
<a class="reference internal image-reference" href="_images/ism_tutorial_3.png"><img alt="_images/ism_tutorial_3.png" src="_images/ism_tutorial_3.png" style="height: 400px;" /></a>
<p>Here the red point represents the predicted center of the object that corresponds to the class of interest.
If you will try to visualize the votes you will see something similar to this image where blue points are votes:</p>
<a class="reference internal image-reference" href="_images/implicit_shape_model.png"><img alt="_images/implicit_shape_model.png" src="_images/implicit_shape_model.png" style="height: 360px;" /></a>
</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Arzoo

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-XXXXXXX-1', 'auto');
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>