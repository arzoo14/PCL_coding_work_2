

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Detecting people on a ground plane with RGB-D data &mdash; PCL DOCUMENTATION 0.0.1 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: dark blue" >
          

          
            <a href="index.html" class="icon icon-home"> PCL DOCUMENTATION
          

          
          </a>

          
            
            
              <div class="version">
                0.0.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Basic Usage</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="walkthrough.html">1. PCL Walkthrough</a></li>
<li class="toctree-l1"><a class="reference internal" href="basic_structures.html">2. Getting Started / Basic Structures</a></li>
<li class="toctree-l1"><a class="reference internal" href="using_pcl_pcl_config.html">3. Using PCL in your own project</a></li>
<li class="toctree-l1"><a class="reference internal" href="compiling_pcl_posix.html">4. Compiling PCL from source on POSIX compliant systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="building_pcl.html">5. Customizing the PCL build process</a></li>
<li class="toctree-l1"><a class="reference internal" href="compiling_pcl_dependencies_windows.html">6. Building PCL’s dependencies from source on Windows</a></li>
<li class="toctree-l1"><a class="reference internal" href="compiling_pcl_windows.html">7. Compiling PCL from source on Windows</a></li>
<li class="toctree-l1"><a class="reference internal" href="compiling_pcl_macosx.html">8. Compiling PCL and its dependencies from MacPorts and source on Mac OS X</a></li>
<li class="toctree-l1"><a class="reference internal" href="installing_homebrew.html">9. Installing on Mac OS X using Homebrew</a></li>
<li class="toctree-l1"><a class="reference internal" href="using_pcl_with_eclipse.html">10. Using PCL with Eclipse</a></li>
<li class="toctree-l1"><a class="reference internal" href="generate_local_doc.html">11. Generate a local documentation for PCL</a></li>
<li class="toctree-l1"><a class="reference internal" href="matrix_transform.html">12. Using a matrix to transform a point cloud</a></li>
</ul>
<p class="caption"><span class="caption-text">Advanced Usage</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="adding_custom_ptype.html">1. Adding your own custom <cite>PointT</cite> type</a></li>
<li class="toctree-l1"><a class="reference internal" href="writing_new_classes.html">2. Writing a new PCL class</a></li>
</ul>
<p class="caption"><span class="caption-text">Features</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="how_features_work.html">1. How 3D Features work in PCL</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">PCL DOCUMENTATION</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Detecting people on a ground plane with RGB-D data</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/ground_based_rgbd_people_detection.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="detecting-people-on-a-ground-plane-with-rgb-d-data">
<span id="ground-based-rgbd-people-detection"></span><h1>Detecting people on a ground plane with RGB-D data<a class="headerlink" href="#detecting-people-on-a-ground-plane-with-rgb-d-data" title="Permalink to this headline">¶</a></h1>
<p>This tutorial aims at explaining how to detect people from RGB-D data with the pcl_people module.
With the proposed method, people standing/walking on a planar ground plane can be detected in real time with standard CPU computation.
This implementation corresponds to the people detection algorithm for RGB-D data presented in</p>
<ul class="simple">
<li><p><em>M. Munaro and E. Menegatti</em>. “Fast RGB-D people tracking for service robots”. In Autonomous Robots, Volume 37 Issue 3, pp. 227-242, Springer, 2014.</p></li>
<li><p><em>M. Munaro, F. Basso and E. Menegatti</em>. “Tracking people within groups with RGB-D data”. In Proceedings of the International Conference on Intelligent Robots and Systems (IROS) 2012, Vilamoura (Portugal), 2012.</p></li>
</ul>
</div>
<div class="section" id="the-code">
<h1>The code<a class="headerlink" href="#the-code" title="Permalink to this headline">¶</a></h1>
<p>You can download the source code for this tutorial from <code class="xref download docutils literal notranslate"><span class="pre">here</span></code>,
while the file containing the needed SVM parameters can be found <code class="xref download docutils literal notranslate"><span class="pre">here</span></code>.
We implemented a people detection demo from a live RGB-D stream obtained with an OpenNI-compatible sensor (Microsoft Kinect, Asus Xtion, etc.).</p>
<p>Here it is the code:</p>
</div>
<div class="section" id="the-explanation">
<h1>The explanation<a class="headerlink" href="#the-explanation" title="Permalink to this headline">¶</a></h1>
<p>Now, let’s break down the code piece by piece.</p>
<p>The first lines allow to print a help text showing the command line parameters that can be set when launching the executable.
No parameter is needed by default, but you can optionally set the path to the file containing the trained SVM
for people detection (<code class="docutils literal notranslate"><span class="pre">--svm</span></code>) and the minimum HOG confidence allowed (<code class="docutils literal notranslate"><span class="pre">--conf</span></code>). Moreover, the minimum (<code class="docutils literal notranslate"><span class="pre">min_h</span></code>) and
maximum (<code class="docutils literal notranslate"><span class="pre">max_h</span></code>) height of people can be set. If no parameter is set, the default values are used.</p>
<p>Here, the callback used for grabbing pointclouds with OpenNI is defined.</p>
<p>The people detection algorithm used makes the assumption that people stand/walk on a planar ground plane.
Thus, it requires to know the equation of the ground plane in order to perform people detection.
In this tutorial, the ground plane is manually initialized by the user by selecting three floor points
from the first acquired pointcloud.
In the following lines, the callback function used for ground plane initialization is shown, together with
the structure used to pass arguments to this callback.</p>
<div class="section" id="main">
<h2>Main:<a class="headerlink" href="#main" title="Permalink to this headline">¶</a></h2>
<p>The main program starts by initializing the main parameters and reading the command line options.</p>
</div>
<div class="section" id="ground-initialization">
<h2>Ground initialization:<a class="headerlink" href="#ground-initialization" title="Permalink to this headline">¶</a></h2>
<p>Then, the <a href="#id1"><span class="problematic" id="id2">:pcl:`pcl::Grabber &lt;pcl::Grabber&gt;`</span></a> object is initialized in order to acquire RGB-D pointclouds and the program waits for
the first frame.
When the first pointcloud is acquired, it is displayed in the visualizer and the user is requested to select
three floor points by pressing <code class="docutils literal notranslate"><span class="pre">shift+click</span></code> as reported in the figure below.
After this, <code class="docutils literal notranslate"><span class="pre">Q</span></code> must be pressed in order to close the visualizer and let the program continue.</p>
<a class="reference internal image-reference" href="_images/Screen_floor.jpg"><img alt="_images/Screen_floor.jpg" class="align-center" src="_images/Screen_floor.jpg" style="height: 300pt;" /></a>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When selecting the floor points, try to click on non collinear points that are distant from each other, in order to improve
plane estimation.</p>
</div>
<p>Given the three points, the ground plane is estimated with a Sample Consensus approach and the plane coefficients are
written to the command window.</p>
<p>In the following lines, we can see the initialization of the SVM classifier by loading the pre-trained parameters
from file.
Moreover, a <a href="#id3"><span class="problematic" id="id4">:pcl:`GroundBasedPeopleDetectionApp &lt;pcl::people::GroundBasedPeopleDetectionApp&gt;`</span></a> object is declared and the main
parameters are set. In this example, we can see how to set the voxel size used for downsampling the pointcloud,
the rgb camera intrinsic parameters, the <a href="#id5"><span class="problematic" id="id6">:pcl:`PersonClassifier &lt;pcl::people::PersonClassifier&gt;`</span></a> object and the height limits.
Other parameters could be set, such as the sensor orientation. If the sensor is vertically placed, the method
setSensorPortraitOrientation should be used to enable the vertical mode in <a href="#id7"><span class="problematic" id="id8">:pcl:`GroundBasedPeopleDetectionApp &lt;pcl::people::GroundBasedPeopleDetectionApp&gt;`</span></a>.</p>
</div>
<div class="section" id="main-loop">
<h2>Main loop:<a class="headerlink" href="#main-loop" title="Permalink to this headline">¶</a></h2>
<p>In the main loop, new frames are acquired and processed until the application is terminated by the user.
The <code class="docutils literal notranslate"><span class="pre">people_detector</span></code> object receives as input the current cloud and the estimated ground coefficients and
computes people clusters properties, which are stored in <a href="#id9"><span class="problematic" id="id10">:pcl:`PersonCluster &lt;pcl::people::PersonCluster&gt;`</span></a> objects.
The ground plane coefficients are re-estimated at every frame by using the previous frame estimate as initial condition.
This procedure allows to adapt to small changes which can occur to the ground plane equation if the camera is slowly moving.</p>
<p>The last part of the code is devoted to visualization. In particular, a green 3D bounding box is drawn for every
person with HOG confidence above the <code class="docutils literal notranslate"><span class="pre">min_confidence</span></code> threshold. The width of the bounding box is fixed, while
the height is determined as the distance between the top point of the person cluster and the ground plane.
The average framerate is also shown every 30 frames, to evaluate the runtime performance of the application.
Please note that this framerate includes the time necessary for grabbing the point clouds and for visualization.</p>
</div>
</div>
<div class="section" id="compiling-and-running-the-program">
<h1>Compiling and running the program<a class="headerlink" href="#compiling-and-running-the-program" title="Permalink to this headline">¶</a></h1>
<p>Create a <cite>CMakeLists.txt</cite> file and add the following lines into it:</p>
<dl class="simple">
<dt>After you have made the executable, you can run it. Simply do:</dt><dd><p>$ ./ground_based_rgbd_people_detector</p>
</dd>
</dl>
<p>The following images show some people detection results on a Kinect RGB-D stream.
The minimum and maximum height for people were set respectively to 1.3 and 2.3 meters, while the
minimum HOG confidence was set to -1.5.</p>
<a class="reference internal image-reference" href="_images/Screen3.jpg"><img alt="_images/Screen3.jpg" src="_images/Screen3.jpg" style="height: 300pt;" /></a>
<a class="reference internal image-reference" href="_images/Screen5.jpg"><img alt="_images/Screen5.jpg" src="_images/Screen5.jpg" style="height: 300pt;" /></a>
<a class="reference internal image-reference" href="_images/Screen8.jpg"><img alt="_images/Screen8.jpg" src="_images/Screen8.jpg" style="height: 300pt;" /></a>
<a class="reference internal image-reference" href="_images/Screen7.jpg"><img alt="_images/Screen7.jpg" src="_images/Screen7.jpg" style="height: 300pt;" /></a>
</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Arzoo

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-XXXXXXX-1', 'auto');
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>