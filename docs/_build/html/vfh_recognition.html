

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Cluster Recognition and 6DOF Pose Estimation using VFH descriptors &mdash; PCL DOCUMENTATION 0.0.1 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: dark blue" >
          

          
            <a href="index.html" class="icon icon-home"> PCL DOCUMENTATION
          

          
          </a>

          
            
            
              <div class="version">
                0.0.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Basic Usage</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="walkthrough.html">1. PCL Walkthrough</a></li>
<li class="toctree-l1"><a class="reference internal" href="basic_structures.html">2. Getting Started / Basic Structures</a></li>
<li class="toctree-l1"><a class="reference internal" href="using_pcl_pcl_config.html">3. Using PCL in your own project</a></li>
<li class="toctree-l1"><a class="reference internal" href="compiling_pcl_posix.html">4. Compiling PCL from source on POSIX compliant systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="building_pcl.html">5. Customizing the PCL build process</a></li>
<li class="toctree-l1"><a class="reference internal" href="compiling_pcl_dependencies_windows.html">6. Building PCL’s dependencies from source on Windows</a></li>
<li class="toctree-l1"><a class="reference internal" href="compiling_pcl_windows.html">7. Compiling PCL from source on Windows</a></li>
<li class="toctree-l1"><a class="reference internal" href="compiling_pcl_macosx.html">8. Compiling PCL and its dependencies from MacPorts and source on Mac OS X</a></li>
<li class="toctree-l1"><a class="reference internal" href="installing_homebrew.html">9. Installing on Mac OS X using Homebrew</a></li>
<li class="toctree-l1"><a class="reference internal" href="using_pcl_with_eclipse.html">10. Using PCL with Eclipse</a></li>
<li class="toctree-l1"><a class="reference internal" href="generate_local_doc.html">11. Generate a local documentation for PCL</a></li>
<li class="toctree-l1"><a class="reference internal" href="matrix_transform.html">12. Using a matrix to transform a point cloud</a></li>
</ul>
<p class="caption"><span class="caption-text">Advanced Usage</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="adding_custom_ptype.html">1. Adding your own custom <cite>PointT</cite> type</a></li>
<li class="toctree-l1"><a class="reference internal" href="writing_new_classes.html">2. Writing a new PCL class</a></li>
</ul>
<p class="caption"><span class="caption-text">Features</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="how_features_work.html">1. How 3D Features work in PCL</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">PCL DOCUMENTATION</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Cluster Recognition and 6DOF Pose Estimation using VFH descriptors</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/vfh_recognition.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="cluster-recognition-and-6dof-pose-estimation-using-vfh-descriptors">
<span id="vfh-recognition"></span><h1>Cluster Recognition and 6DOF Pose Estimation using VFH descriptors<a class="headerlink" href="#cluster-recognition-and-6dof-pose-estimation-using-vfh-descriptors" title="Permalink to this headline">¶</a></h1>
<p>As previously described in <a class="reference internal" href="vfh_estimation.html#vfh-estimation"><span class="std std-ref">Estimating VFH signatures for a set of points</span></a>, Viewpoint Feature Histograms
(VFH) are powerful <em>meta-local</em> descriptors, created for the purpose of
recognition and pose estimation for <strong>clusters</strong> of points. We here refer to a
<strong>cluster</strong> as a collection of 3D points, most of the time representing a
particular object or part of a scene, obtained through some segmentation or
detection mechanisms (please see <a class="reference internal" href="cluster_extraction.html#cluster-extraction"><span class="std std-ref">Euclidean Cluster Extraction</span></a> for an example).</p>
<p>Our goal here is not to provide an ultimate recognition tool, but rather a
mechanism for obtaining <strong>candidates</strong> that <em>could potentially be the
cluster/object that is searched for</em>, together with its 6DOF pose in space.
With this in mind, we will be formulating the <em>recognition</em> problem as a
<em>nearest neighbor estimation</em> problem. So given a set of <em>training data</em>, we
will use efficient nearest neighbor search structures such as <em>kd-trees</em> and
return a set of potential candidates with sorted distances to the query object,
rather than an absolute <em>“this is the object that we were searching for”</em> kind
of response. The reader can imagine that such a system becomes much more useful
as we can explicitly reason about failures (false positives, or true
negatives).</p>
<p>For the purpose of this tutorial, the application example could be formulated as follows:</p>
<blockquote>
<div><ul class="simple">
<li><p>Training stage:</p>
<ul>
<li><p>given a scene with 1 object that is easily separable as a cluster;</p></li>
<li><p>use a ground-truth system to obtain its pose (see the discussion below);</p></li>
<li><p>rotate around the object or rotate the object with respect to the camera, and compute a VFH descriptor for each view;</p></li>
<li><p>store the views, and build a kd-tree representation.</p></li>
</ul>
</li>
<li><p>Testing stage:</p>
<ul>
<li><p>given a scene with objects that can be separated as individual clusters, first extract the clusters;</p></li>
<li><p>for each cluster, compute a VFH descriptor from the current camera position;</p></li>
<li><p>use the VFH descriptor to search for candidates in the trained kd-tree.</p></li>
</ul>
</li>
</ul>
</div></blockquote>
<p>We hope the above makes sense. Basically we’re first going to create the set of
objects that we try to later on recognize, and then we will use that to obtain
valid candidates for objects in the scene.</p>
<p>A good example of a ground-truth system could be a simple rotating pan-tilt
unit such as the one in the figure below. Placing an object on the unit, and
moving it with some increments in both horizontal and vertical, can result in a
perfect ground-truth system for small objects. A cheaper solution could be to
use a marker-based system (e.g., checkerboard) and rotate the camera/table
manually.</p>
<img alt="_images/pan_tilt.jpg" class="align-center" src="_images/pan_tilt.jpg" />
<p>Our Kd-Tree implementation of choice for the purpose of this tutorial is of
course, <a class="reference external" href="http://www.cs.ubc.ca/research/flann/">FLANN</a>.</p>
</div>
<div class="section" id="training">
<h1>Training<a class="headerlink" href="#training" title="Permalink to this headline">¶</a></h1>
<p>We begin the training by assuming that the <em>objects</em> are already separated as
individual clusters (see <a class="reference internal" href="cluster_extraction.html#cluster-extraction"><span class="std std-ref">Euclidean Cluster Extraction</span></a>), as shown in the figure
below:</p>
<img alt="_images/scene_raw.jpg" src="_images/scene_raw.jpg" />
<img alt="_images/scene_segmented.jpg" src="_images/scene_segmented.jpg" />
<p>Since we’re only trying to cover the explicit training/testing of VFH
signatures in this tutorial, we provide a set of datasets already collected at:
<a class="reference external" href="https://raw.github.com/PointCloudLibrary/data/master/tutorials/vfh_recognition/vfh_recognition_tutorial_data.tbz">vfh_recognition_tutorial_data.tbz</a>.
The data is a subset of the objects presented in the figure below (left), and
look like the point clouds on the right. We used the pan-tilt table shown above
to acquire the data.</p>
<img alt="_images/objects.jpg" src="_images/objects.jpg" />
<img alt="_images/training.jpg" src="_images/training.jpg" />
<p>Next, copy and paste the following code into your editor and save it as
<code class="docutils literal notranslate"><span class="pre">build_tree.cpp</span></code>.</p>
<p>In the following paragraphs we will explain what the above code does (or should
do). We’ll begin with the <code class="docutils literal notranslate"><span class="pre">main</span></code> function.</p>
<p>We begin by loading a set of feature models from a directory given as the first
command line argument (see details for running the example below). The
<code class="docutils literal notranslate"><span class="pre">loadFeatureModels</span></code> method does nothing but recursively traverse a set of
directories and subdirectories, and loads in all <em>.PCD</em> files it finds. In
<code class="docutils literal notranslate"><span class="pre">loadFeatureModels</span></code>, we call <code class="docutils literal notranslate"><span class="pre">loadHist</span></code>, which will attempt to open each
PCD file found, read its header, and check whether it contains a VFH signature
or not. Together with the VFH signature we also store the PCD file name into a
<code class="docutils literal notranslate"><span class="pre">vfh_model</span></code> pair.</p>
<p>Once all VFH features have been loaded, we convert them to FLANN format, using:</p>
<p>Since we’re lazy, and we want to use this data (and not reload it again by crawling the directory structure in the testing phase), we dump the data to disk:</p>
<p>Finally, we create the KdTree, and save its structure to disk:</p>
<p>Here we will use a <code class="docutils literal notranslate"><span class="pre">LinearIndex</span></code>, which does a brute-force search using a
Chi-Square distance metric (see <a class="reference internal" href="vfh_estimation.html#vfh" id="id1"><span>[VFH]</span></a> for more information). For building a
proper kd-tree, comment line 1 and uncomment line 2 in the code snippet above.
The most important difference between a LinearIndex and a KDTreeIndex in FLANN
is that the KDTree will be much faster, while producing approximate nearest
neighbor results, rather than absolute.</p>
<p>So, we’re done with training. To summarize:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>we crawled a directory structure, looked at all the .PCD files we found, tested them whether they are VFH signatures and loaded them in memory;</p></li>
<li><p>we converted the data into FLANN format and dumped it to disk;</p></li>
<li><p>we built a kd-tree structure and dumped it to disk.</p></li>
</ol>
</div></blockquote>
</div>
<div class="section" id="testing">
<h1>Testing<a class="headerlink" href="#testing" title="Permalink to this headline">¶</a></h1>
<p>In the testing phase, we will illustrate how the system works by randomly
loading one of the files used in the training phase (feel free to supply your
own file here!), and checking the results of the tree.</p>
<p>Begin by copying and pasting the following code into your editor and save it as
<code class="docutils literal notranslate"><span class="pre">nearest_neighbors.cpp</span></code>.</p>
<p>The above code snippet is slightly larger, because we also included some
visualization routines and some other “eye candy” stuff.</p>
<p>In lines:</p>
<p>we load the first given user histogram (and ignore the rest). Then we proceed
at checking two command line parameters, namely <code class="docutils literal notranslate"><span class="pre">-k</span></code> which will define how
many nearest neighbors to check and display on screen, and <code class="docutils literal notranslate"><span class="pre">-thresh</span></code> which
defines a maximum distance metric after which we will start displaying red
lines (i.e., crossing) over the <strong>k</strong> models found on screen (eye candy!).</p>
<p>In lines:</p>
<p>we load the training data from disk, together with the list of file names that
we previously stored in <code class="docutils literal notranslate"><span class="pre">build_tree.cpp</span></code>. Then, we read the kd-tree and rebuild the index:</p>
<p>Here we need to make sure that we use the <strong>exact</strong> distance metric
(<code class="docutils literal notranslate"><span class="pre">ChiSquareDistance</span></code> in this case), as the one that we used while creating
the tree. The most important part of the code comes here:</p>
<p>Inside <code class="docutils literal notranslate"><span class="pre">nearestKSearch</span></code>, we first convert the query point to FLANN format:</p>
<p>Followed by obtaining the resultant nearest neighbor indices and distances for the query in:</p>
<p>Lines:</p>
<p>create a <code class="docutils literal notranslate"><span class="pre">PCLVisualizer</span></code> object, and sets up a set of different viewports (e.g., splits the screen into different chunks), which will be enabled in:</p>
<p>Using the file names representing the models that we previously obtained in
<code class="docutils literal notranslate"><span class="pre">loadFileList</span></code>, we proceed at loading the model file names using:</p>
<p>For visualization purposes, we demean the point cloud by computing its centroid and then subtracting it:</p>
<p>Finally we check if the distance obtained by <code class="docutils literal notranslate"><span class="pre">nearestKSearch</span></code> is larger than the user given threshold, and if it is, we display a red line over the cloud that is being rendered in the viewport:</p>
</div>
<div class="section" id="compiling-and-running-the-code">
<h1>Compiling and running the code<a class="headerlink" href="#compiling-and-running-the-code" title="Permalink to this headline">¶</a></h1>
<p>Create a new <code class="docutils literal notranslate"><span class="pre">CMakeLists.txt</span></code> file, and put the following content into it</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you are running this tutorial on Windows, you have to install (<a class="reference external" href="http://www.hdfgroup.org/ftp/HDF5/current/bin/windows/">HDF5 1.8.7 Shared Library</a>). If CMake is not able to find HDF5,
you can manually supply the include directory in HDF5_INCLUDE_DIR variable and the full path of <strong>hdf5dll.lib</strong> in HDF5_hdf5_LIBRARY variable.
Make sure that the needed dlls are in the same folder as the executables.</p>
</div>
<p>The above assumes that your two source files (<code class="docutils literal notranslate"><span class="pre">build_tree.cpp</span></code> and <code class="docutils literal notranslate"><span class="pre">nearest_neighbors.cpp</span></code>) are stored into the <em>src/</em> subdirectory.</p>
<p>Then, make sure that the datasets you downloaded (<a class="reference external" href="https://raw.github.com/PointCloudLibrary/data/master/tutorials/vfh_recognition/vfh_recognition_tutorial_data.tbz">vfh_recognition_tutorial_data.tbz</a>) are unpacked in this directory, thus creating a <em>data/</em> subdirectory.</p>
<p>After you have made the executable, you can run them like so:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ ./build/build_tree data/
</pre></div>
</div>
<p>You should see the following output on screen:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">&gt;</span> <span class="n">Loading</span> <span class="n">data</span><span class="o">/</span><span class="mf">001.324</span><span class="o">.</span><span class="mi">25</span> <span class="p">(</span><span class="mi">0</span> <span class="n">models</span> <span class="n">loaded</span> <span class="n">so</span> <span class="n">far</span><span class="p">)</span><span class="o">.</span>
<span class="o">&gt;</span> <span class="n">Loading</span> <span class="n">data</span><span class="o">/</span><span class="mf">800.919</span><span class="o">.</span><span class="mi">49</span> <span class="p">(</span><span class="mi">13</span> <span class="n">models</span> <span class="n">loaded</span> <span class="n">so</span> <span class="n">far</span><span class="p">)</span><span class="o">.</span>
<span class="o">&gt;</span> <span class="n">Loading</span> <span class="n">data</span><span class="o">/</span><span class="mf">100.922</span><span class="o">.</span><span class="mi">16</span> <span class="p">(</span><span class="mi">27</span> <span class="n">models</span> <span class="n">loaded</span> <span class="n">so</span> <span class="n">far</span><span class="p">)</span><span class="o">.</span>
<span class="o">&gt;</span> <span class="n">Loading</span> <span class="n">data</span><span class="o">/</span><span class="mf">901.125</span><span class="o">.</span><span class="mi">07</span> <span class="p">(</span><span class="mi">47</span> <span class="n">models</span> <span class="n">loaded</span> <span class="n">so</span> <span class="n">far</span><span class="p">)</span><span class="o">.</span>
<span class="o">&gt;</span> <span class="n">Loading</span> <span class="n">data</span><span class="o">/</span><span class="mf">000.580</span><span class="o">.</span><span class="mi">67</span> <span class="p">(</span><span class="mi">65</span> <span class="n">models</span> <span class="n">loaded</span> <span class="n">so</span> <span class="n">far</span><span class="p">)</span><span class="o">.</span>
<span class="o">&gt;</span> <span class="n">Loading</span> <span class="n">data</span><span class="o">/</span><span class="mf">463.156</span><span class="o">.</span><span class="mi">00</span> <span class="p">(</span><span class="mi">81</span> <span class="n">models</span> <span class="n">loaded</span> <span class="n">so</span> <span class="n">far</span><span class="p">)</span><span class="o">.</span>
<span class="o">&gt;</span> <span class="n">Loading</span> <span class="n">data</span><span class="o">/</span><span class="mf">401.431</span><span class="o">.</span><span class="mi">44</span> <span class="p">(</span><span class="mi">97</span> <span class="n">models</span> <span class="n">loaded</span> <span class="n">so</span> <span class="n">far</span><span class="p">)</span><span class="o">.</span>
<span class="o">&gt;</span> <span class="n">Loading</span> <span class="n">data</span><span class="o">/</span><span class="mf">100.919</span><span class="o">.</span><span class="mi">00</span> <span class="p">(</span><span class="mi">113</span> <span class="n">models</span> <span class="n">loaded</span> <span class="n">so</span> <span class="n">far</span><span class="p">)</span><span class="o">.</span>
<span class="o">&gt;</span> <span class="n">Loading</span> <span class="n">data</span><span class="o">/</span><span class="mf">401.324</span><span class="o">.</span><span class="mi">52</span> <span class="p">(</span><span class="mi">134</span> <span class="n">models</span> <span class="n">loaded</span> <span class="n">so</span> <span class="n">far</span><span class="p">)</span><span class="o">.</span>
<span class="o">&gt;</span> <span class="n">Loading</span> <span class="n">data</span><span class="o">/</span><span class="mf">201.327</span><span class="o">.</span><span class="mi">78</span> <span class="p">(</span><span class="mi">150</span> <span class="n">models</span> <span class="n">loaded</span> <span class="n">so</span> <span class="n">far</span><span class="p">)</span><span class="o">.</span>
<span class="o">&gt;</span> <span class="n">Loading</span> <span class="n">data</span><span class="o">/</span><span class="mf">300.151</span><span class="o">.</span><span class="mi">23</span> <span class="p">(</span><span class="mi">166</span> <span class="n">models</span> <span class="n">loaded</span> <span class="n">so</span> <span class="n">far</span><span class="p">)</span><span class="o">.</span>
<span class="o">&gt;</span> <span class="n">Loading</span> <span class="n">data</span><span class="o">/</span><span class="mf">200.921</span><span class="o">.</span><span class="mi">07</span> <span class="p">(</span><span class="mi">180</span> <span class="n">models</span> <span class="n">loaded</span> <span class="n">so</span> <span class="n">far</span><span class="p">)</span><span class="o">.</span>
<span class="o">&gt;</span> <span class="n">Loaded</span> <span class="mi">195</span> <span class="n">VFH</span> <span class="n">models</span><span class="o">.</span> <span class="n">Creating</span> <span class="n">training</span> <span class="n">data</span> <span class="n">training_data</span><span class="o">.</span><span class="n">h5</span><span class="o">/</span><span class="n">training_data</span><span class="o">.</span><span class="n">list</span><span class="o">.</span>
<span class="n">Building</span> <span class="n">the</span> <span class="n">kdtree</span> <span class="n">index</span> <span class="p">(</span><span class="n">kdtree</span><span class="o">.</span><span class="n">idx</span><span class="p">)</span> <span class="k">for</span> <span class="mi">195</span> <span class="n">elements</span><span class="o">...</span>
</pre></div>
</div>
<p>The above crawled the <em>data/</em> subdirectory, and created a kd-tree with 195 entries. To run the nearest neighbor testing example, you have two options:</p>
<blockquote>
<div><ol class="arabic">
<li><p>Either run the following command manually, and select one of the datasets that we provided as a testing sample, like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">./</span><span class="n">build</span><span class="o">/</span><span class="n">nearest_neighbors</span> <span class="o">-</span><span class="n">k</span> <span class="mi">16</span> <span class="o">-</span><span class="n">thresh</span> <span class="mi">50</span> <span class="n">data</span><span class="o">/</span><span class="mf">000.580</span><span class="o">.</span><span class="mi">67</span><span class="o">/</span><span class="mi">1258730231333</span><span class="n">_cluster_0_nxyz_vfh</span><span class="o">.</span><span class="n">pcd</span>
</pre></div>
</div>
</li>
<li><p>Or, if you are on a linux system, you can place the following on a bash script file (e.g., <code class="docutils literal notranslate"><span class="pre">test.sh</span></code>):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>#!/bin/bash

# Example directory containing _vfh.pcd files
DATA=data

# Inlier distance threshold
thresh=50

# Get the closest K nearest neighbors
k=16

for i in `find $DATA -type d -name &quot;*&quot;`
do
  echo $i
  for j in `find $i -type f \( -iname &quot;*cluster*_vfh.pcd&quot; \) | sort -R`
  do
    echo $j
    ./build/nearest_neighbors -k $k -thresh $thresh $j -cam &quot;0.403137,0.868471/0,0,0/-0.0932051,-0.201608,-0.518939/-0.00471487,-0.931831,0.362863/1464,764/6,72&quot;
  done
done
</pre></div>
</div>
</li>
</ol>
<blockquote>
<div><p>and run the script like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">bash</span> <span class="n">test</span><span class="o">.</span><span class="n">sh</span>
</pre></div>
</div>
</div></blockquote>
</div></blockquote>
<p>You should see <em>recognition</em> examples like the ones shown below:</p>
<img alt="_images/vfh_example1.jpg" src="_images/vfh_example1.jpg" />
<img alt="_images/vfh_example2.jpg" src="_images/vfh_example2.jpg" />
<img alt="_images/vfh_example3.jpg" src="_images/vfh_example3.jpg" />
</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Arzoo

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-XXXXXXX-1', 'auto');
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>